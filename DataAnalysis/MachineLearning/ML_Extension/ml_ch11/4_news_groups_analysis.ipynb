{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 newsgroups Dataset\n",
    "- 대표적인 Text 분류 Toy dataset\n",
    "- 20개의 뉴스 테스트 데이터를 분류하라 !\n",
    "- Multiclass classification\n",
    "- 약 20,000개의 news document 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 분류기 모델 만들기\n",
    "\n",
    "* 데이터 파악\n",
    "* 전처리(Preprocessing)\n",
    "\n",
    "    * 필요없는 단어 제거 (Data Cleansing)\n",
    "    * CountVectorizer & Tf-idfVectorizer\n",
    "\n",
    "---\n",
    "    \n",
    "* Modeling : BernoulliNB, MultinomialNB 사용\n",
    "  * Cross Validation(Kfold 이용)\n",
    "  \n",
    "---\n",
    "\n",
    "* Pipeline 이용\n",
    "\n",
    "---\n",
    "\n",
    "* Assignment Description\n",
    "     * 위 신문 데이터를 바탕으로 신문 내용별 분류기를 개발하라\n",
    "     * 위 데이터를 Traing / Test Dataset으로 나눠서 5-fold cross validation(5번 데이터를 training / testset으로 나눔, KV 활용)\n",
    "     * Naive Bayesian Classifier와 Count Vector를 활용하여 각각 성능을 테스트하라\n",
    "         * NB는 multinomial과 bernuoil 분포를 모두 사용하라\n",
    "     * 가능할 경우, TF-IDF vector를 활용해 볼것 (검색어 - tf-idf scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "   * 18846개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...          rec.sport.hockey\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...  comp.sys.ibm.pc.hardware\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...     talk.politics.mideast\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...  comp.sys.ibm.pc.hardware\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target 데이터 -> 문자 라벨링(뉴스마다 어떤 뉴스인지 보기 편하도록 만들기 위해서)\n",
    "def word_labeling(lst, df):\n",
    "    for idx, name in enumerate(lst):\n",
    "        target_data = df['Target']\n",
    "        for idx_, num_label in enumerate(target_data):\n",
    "            if num_label == idx:\n",
    "                df.loc[idx_, 'Target'] = name\n",
    "    return df\n",
    "news_df = word_labeling(news['target_names'], news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Cleansing\n",
    "    * 이메일 제거\n",
    "    * 불필요 숫자 제거\n",
    "    * 문자 아닌 특수문자 제거\n",
    "    * 단어 사이 공백 제거 : 띄어쓰기 별로 split해주고 join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(df):\n",
    "    delete_email = re.sub(r'\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b', ' ', df)\n",
    "    delete_number = re.sub(r'\\b|\\d+|\\b', ' ',delete_email)\n",
    "    delete_non_word = re.sub(r'\\b[\\W]+\\b', ' ', delete_number)\n",
    "    cleaning_result = ' '.join(delete_non_word.split())\n",
    "    return cleaning_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Mamatha Devineni Ratnam Subject Pens fans...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Matthew B Lawson Subject Which high perfo...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Guy Dawson Subject Re IDE vs SCSI DMA and...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From Alexander Samuel McDiarmid Subject driver...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From Mamatha Devineni Ratnam Subject Pens fans...          rec.sport.hockey\n",
       "1  From Matthew B Lawson Subject Which high perfo...  comp.sys.ibm.pc.hardware\n",
       "2  From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...     talk.politics.mideast\n",
       "3  From Guy Dawson Subject Re IDE vs SCSI DMA and...  comp.sys.ibm.pc.hardware\n",
       "4  From Alexander Samuel McDiarmid Subject driver...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer\n",
    "* CountVectorizer \n",
    "  * 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만듦\n",
    "* TfidfVectorizer \n",
    "    * 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법\n",
    "    * TF(Term Frequency) : 문서에서 해당 단어가 얼마나 나왔는지 나타내주는 빈도 수\n",
    "    * DF(Document Frequency) : 해당 단어가 있는 문서의 수\n",
    "    * IDF(Inverse Document Frequency) 해당 단어가 있는 문서의 수가 높아질 수록 가중치를 축소해주기 위해 역수 취해줌\n",
    "        * log(N / (1 + DF))      \n",
    "            * N : 전체 문서의 수\n",
    "    * TF-IDF = TF * IDF\n",
    "* CustomizedVectorizer - StemmedCounterVectorizer, StemmedTfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'look', 'look']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import stem\n",
    "stmmer = stem.SnowballStemmer(\"english\")\n",
    "sentence = 'looking looks looked'\n",
    "[stmmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imag', 'imag', 'imagin')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmmer.stem(\"images\"), stmmer.stem(\"imaging\"), stmmer.stem(\"imagination\")  # 명사, 동사 구분 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import  nltk\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer): # CountVectorizer의 자식 class \n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer() # CountVectorizer의 analyzer 선언\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc)) # Stemming 만 추가로 적용하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'look': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StemmedCountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'looking': 1, 'looks': 2, 'looked': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "* Pipeline\n",
    "* Gridsearch\n",
    "* Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), 3)\n",
    "for grams in sixgrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# GaussianNB 알고리즘 - Dense한 데이터를 입력값으로 가짐\n",
    "# vectorizer의 output : CSR form\n",
    "# CSR form을 Dense form으로 변환하기 위한 class\n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Plan - Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer\n",
    "- Tfidf\n",
    "- Count\n",
    "- StemmedTfidf\n",
    "- StemmedCount\n",
    "\n",
    "Algorithm\n",
    "- LogisticRegression\n",
    "- Bernoulli NB\n",
    "- Multinomial NB\n",
    "- Gaussian NB\n",
    "\n",
    "Metrics\n",
    "- CV 2 times\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                 ('multinomialnb', MultinomialNB())]),\n",
       " Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                 ('bernoullinb', BernoulliNB())]),\n",
       " Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                 ('multinomialnb', MultinomialNB())]),\n",
       " Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                 ('bernoullinb', BernoulliNB())]),\n",
       " Pipeline(steps=[('stemmedcountvectorizer', StemmedCountVectorizer()),\n",
       "                 ('multinomialnb', MultinomialNB())]),\n",
       " Pipeline(steps=[('stemmedcountvectorizer', StemmedCountVectorizer()),\n",
       "                 ('bernoullinb', BernoulliNB())]),\n",
       " Pipeline(steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer()),\n",
       "                 ('multinomialnb', MultinomialNB())]),\n",
       " Pipeline(steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer()),\n",
       "                 ('bernoullinb', BernoulliNB())])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer(), StemmedCountVectorizer(), StemmedTfidfVectorizer()]\n",
    "# algorithms = [BernoulliNB(), MultinomialNB(), GaussianNB(), LogisticRegression()]\n",
    "algorithms = [MultinomialNB(), BernoulliNB()]\n",
    "\n",
    "pipelines  = []\n",
    "\n",
    "\n",
    "import itertools\n",
    "for case in list(itertools.product(vectorizer, algorithms)): # 모든 경우의 수를 리스트로 만들기\n",
    "    if isinstance(case[1], GaussianNB): # case[1](=알고리즘)이 GaussianNB instance이면\n",
    "        case = list(case)\n",
    "        case.insert(1,  DenseTransformer()) # CSR form을 Dense한 데이터로 transform 하도록 함\n",
    "        # case의 형태 : [vectorizer, DenseTransformer(), GaussianNB()]\n",
    "    pipelines.append(make_pipeline(*case))\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer Common params\n",
    "ngrams_params = [(1,1),(1,3)]\n",
    "stopword_params = [\"english\"]\n",
    "lowercase_params = [True, False]\n",
    "max_df_params = np.linspace(0.4, 0.6, num=4)\n",
    "min_df_params = np.linspace(0.0, 0.1, num=4)\n",
    "\n",
    "attributes = {\"ngram_range\":ngrams_params, \"max_df\":max_df_params,\"min_df\":min_df_params,\n",
    "              \"lowercase\":lowercase_params,\"stop_words\":stopword_params}\n",
    "vectorizer_names = [\"countvectorizer\",\"tfidfvectorizer\",\"stemmedcountvectorizer\",\"stemmedtfidfvectorizer\"]\n",
    "vectorizer_params_dict = {}\n",
    "\n",
    "for vect_name in vectorizer_names:\n",
    "    vectorizer_params_dict[vect_name] = {}\n",
    "    for key, value in attributes.items():\n",
    "        param_name = vect_name + \"__\" + key\n",
    "        vectorizer_params_dict[vect_name][param_name] =  value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ngram_range', [(1, 1), (1, 3)]), ('max_df', array([0.4       , 0.46666667, 0.53333333, 0.6       ])), ('min_df', array([0.        , 0.03333333, 0.06666667, 0.1       ])), ('lowercase', [True, False]), ('stop_words', ['english'])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer': {'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'countvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english']},\n",
       " 'tfidfvectorizer': {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'tfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english']},\n",
       " 'stemmedcountvectorizer': {'stemmedcountvectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english']},\n",
       " 'stemmedtfidfvectorizer': {'stemmedtfidfvectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english']}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomialnb': {'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " 'bernoullinb': {'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Algorithms parameters\n",
    "# algorithm_names = [\"bernoullinb\",\"multinomialnb\",\"gaussiannb\",\"logisticregression\"]\n",
    "algorithm_names = [\"multinomialnb\", \"bernoullinb\"]\n",
    "\n",
    "algorithm_params_dict = {}\n",
    "\n",
    "\n",
    "#'bernoullinb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])\n",
    "alpha_params = np.linspace(1.0, 2.0, num=5)\n",
    "for i in range(2):\n",
    "    algorithm_params_dict[algorithm_names[i]] = {\n",
    "    algorithm_names[i]+ \"__alpha\" : alpha_params    \n",
    "    }\n",
    "# algorithm_params_dict[algorithm_names[2]] = {}\n",
    "\n",
    "\n",
    "# LogisticRegression    \n",
    "# multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’\n",
    "# C : float, default: 1.0\n",
    "# solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "# n_jobs : int, default: 1\n",
    "# penalty : str, ‘l1’ or ‘l2’, default: ‘l2’\n",
    "\n",
    "# multi_class_params = [\"ovr\", \"multinomial\"]\n",
    "# c_params = [0.1,  5.0, 7.0, 10.0, 15.0, 20.0, 100.0]\n",
    "# algorithm_params_dict[algorithm_names[1]] = [{\n",
    "#     \"logisticregression__multi_class\" : [\"multinomial\"],\n",
    "#     \"logisticregression__solver\" : [\"saga\"],\n",
    "#     \"logisticregression__penalty\" : [\"l1\"],\n",
    "#     \"logisticregression__C\" : c_params\n",
    "#     # },{\n",
    "#     # \"logisticregression__multi_class\" : [\"ovr\"],\n",
    "#     # \"logisticregression__solver\" : ['liblinear'],\n",
    "#     # \"logisticregression__penalty\" : [\"l2\"],\n",
    "#     # \"logisticregression__C\" : c_params\n",
    "#     }\n",
    "#     ]\n",
    "algorithm_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'countvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'countvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__stop_words': ['english'],\n",
       "  'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'tfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'tfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__stop_words': ['english'],\n",
       "  'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedcountvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'stemmedcountvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__stop_words': ['english'],\n",
       "  'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
       " {'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedtfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english'],\n",
       "  'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_params= []\n",
    "for case in list(itertools.product(vectorizer_names, algorithm_names)):\n",
    "    vect_params = vectorizer_params_dict[case[0]].copy()\n",
    "    algo_params = algorithm_params_dict[case[1]]\n",
    "    \n",
    "    # 두 params를 한번에 넣어주기 위해 합치기\n",
    "    if isinstance(algo_params, dict): # algo_params이 하나의 dict 형태인 경우\n",
    "        vect_params.update(algo_params)\n",
    "        pipeline_params.append(vect_params) # 합쳐진 params를 pipeline_parms에 넣기\n",
    "    else: # algo_params이 list[dict] 형태인 경우\n",
    "        temp = []\n",
    "        for param in algo_params:\n",
    "            vect_params.update(param)\n",
    "            temp.append(vect_params)\n",
    "        pipeline_params.append(temp)\n",
    "pipeline_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn! Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17,  3,  4, 12,  4, 10, 10, 19, 19, 11, 19, 13,  0, 17, 12,\n",
       "       12, 11,  8,  7,  5,  1,  8, 10, 14, 16,  1,  6,  0,  7, 16,  5,  9,\n",
       "       13,  4,  4, 18,  8,  8, 19,  1, 12,  7, 10,  5,  2,  6, 11,  2, 12,\n",
       "        7, 18, 11,  7,  8,  0,  4, 19,  8,  9,  4,  1,  1, 17, 11, 10, 11,\n",
       "        5,  1,  3, 17,  6, 14, 19, 14, 10,  2, 15, 10, 12,  7,  5, 12,  4,\n",
       "       15, 16, 13,  8, 15,  9, 19, 15,  2, 17,  3,  2, 10, 16,  5, 17,  1,\n",
       "        2, 17, 19, 10,  4, 18,  6,  2,  4,  7, 14,  5, 17, 17, 12, 11,  9,\n",
       "        4,  3, 12,  2, 12, 16,  4,  2, 10, 12,  1, 17, 15, 16, 10,  3, 17,\n",
       "        2, 11,  3,  8,  0,  2,  7,  5, 13, 11,  4,  1,  9,  6,  8,  8,  3,\n",
       "        3, 18,  4,  8, 18, 14,  4,  1,  3, 12, 15,  1,  4, 15, 14,  4, 13,\n",
       "        4, 13,  3,  0,  5,  2,  9, 15,  8, 10,  7,  9, 18, 12,  4,  2, 17,\n",
       "       17, 17, 19,  6, 15, 11,  0, 19, 15, 17,  6, 13,  1, 19,  4, 15,  5,\n",
       "        1,  7,  6, 15, 12,  0,  7,  5,  5,  3,  8, 16,  2, 17, 14, 11, 10,\n",
       "       15,  3,  7, 11,  0, 11,  8,  8, 10,  8,  3, 14,  3,  9,  5, 15, 10,\n",
       "       10,  5, 12, 12, 13, 14,  6,  6,  2,  7, 15,  0,  3, 12,  1,  3, 16,\n",
       "        5, 12, 16,  4,  1,  3, 12, 14, 13,  2,  2,  1, 14,  8,  3,  1, 13,\n",
       "       17,  2, 13, 19, 17,  5, 11, 15, 13, 19, 19,  9,  5, 16, 17, 13,  4,\n",
       "        3,  2, 16, 18,  5,  1,  5,  6, 16,  5, 11, 15,  0,  1, 18,  7,  5,\n",
       "        4, 18, 10, 14,  4,  7,  0, 12,  7,  1, 13, 17, 12,  0, 16,  3, 15,\n",
       "        2,  3,  9, 14,  9,  2,  8, 11,  2,  3,  9,  1,  5, 19, 14,  2, 14,\n",
       "        1, 18,  5,  0, 18, 15, 16, 17,  0, 13,  3, 16, 13,  6, 17, 15, 10,\n",
       "        5, 13,  2,  1,  7,  0, 10, 18, 10, 14, 15, 16,  3,  1, 10,  7, 16,\n",
       "        0, 10, 19, 19,  3, 12, 11, 15,  5, 13, 14, 10,  9,  5,  7,  7, 14,\n",
       "       15,  7,  9,  9, 18,  5,  1, 15,  5,  2, 19, 10,  0, 13, 12, 19, 12,\n",
       "       13, 14,  9,  3, 15,  8, 11, 14, 16, 16,  6, 15, 12, 12,  3, 12, 16,\n",
       "       15, 12, 10, 11,  8,  1, 17,  9, 13,  8, 19,  2,  4,  3, 16,  8,  1,\n",
       "       13, 15,  4, 13, 11,  4,  7,  1,  6, 13, 15, 14,  0,  2,  3,  8, 10,\n",
       "        4, 10,  6,  5,  6,  5,  6,  4,  2, 15,  4,  6, 18,  1,  5,  9, 19,\n",
       "        4,  0,  1, 19,  0,  3, 11,  4,  2,  4,  9,  0, 12, 19,  7,  7,  2,\n",
       "        6, 11, 14,  7, 18,  9,  3, 15,  2,  1,  1,  8, 14,  3,  6, 11,  7,\n",
       "        1, 14, 19,  9, 10,  5, 12, 12,  4,  8, 12, 17, 14,  1, 16,  9,  0,\n",
       "        9, 13,  9, 14, 13,  7, 16,  2,  9,  5, 13, 18,  4,  8,  8,  9,  3,\n",
       "       15, 16,  1,  8, 13, 15, 16,  5, 17,  8,  9,  5, 10,  8, 15,  8,  7,\n",
       "       17,  0, 10,  3, 12,  8, 11,  5,  8, 18,  6,  5, 18,  8, 11,  9, 15,\n",
       "       17,  8, 13, 12,  8,  0, 19,  2, 17, 14, 10,  7, 18, 16, 17, 10, 12,\n",
       "       16,  1,  8, 10, 11,  1, 14, 16,  1,  2, 17, 12, 11,  6,  4, 14,  6,\n",
       "       15,  6, 15, 12, 11, 19, 12,  6, 16,  1,  6,  9,  5, 13, 17, 14,  5,\n",
       "       10,  9, 16, 14,  0,  6,  0,  0, 18, 19, 16,  6, 18, 18,  3, 14, 13,\n",
       "       13,  2, 19, 10,  8,  2, 19,  5, 18, 15,  5, 14,  5,  5, 12,  6,  7,\n",
       "        6, 19, 18,  9, 19, 18,  4, 10,  3,  0,  5, 11, 12, 12,  0, 11, 13,\n",
       "       15,  3,  5,  2,  8,  7,  9, 10, 18, 13,  5, 16,  0, 14,  6,  4,  3,\n",
       "        3, 14,  1, 11,  9,  3,  1,  1, 19, 13, 10, 17,  5,  9,  3, 11,  7,\n",
       "       11, 16,  3, 12, 12,  4, 13, 14, 17, 14,  1,  2, 16, 12,  1,  2, 19,\n",
       "       17, 19, 13, 16,  0, 17, 18,  9,  6, 14,  2, 10, 17,  9,  8,  8, 10,\n",
       "        5,  6,  5, 12,  5,  4,  0,  3,  5, 13, 13, 10,  7, 18, 13, 10,  1,\n",
       "       16, 19, 15,  2, 18, 18,  2, 12,  7, 11, 10, 10,  3, 12, 15,  0, 13,\n",
       "        5,  6, 14,  7, 14, 11,  3,  4,  9, 15,  9,  3,  6,  8, 18,  2, 12,\n",
       "       11,  7,  3, 15,  8,  4,  8,  2, 11,  3,  1, 14, 19, 11, 10,  3, 12,\n",
       "       11,  5, 12,  7, 10,  0,  5,  3,  1, 10, 15,  1,  9,  0, 15,  1,  0,\n",
       "       12,  2, 15, 11,  1,  7, 14, 18,  3,  4,  9, 13,  3,  0, 14,  6, 11,\n",
       "       18,  4, 16, 15, 14,  5, 10, 15, 17, 16, 10,  5, 12,  4,  7,  6,  6,\n",
       "       13, 19, 15, 16, 18,  4,  4,  4,  3, 13,  4, 10,  5, 13,  0,  1,  3,\n",
       "        0, 19, 15,  0,  0,  7, 14, 13, 12, 14,  2, 19,  5, 11, 14,  9, 11,\n",
       "        9, 14, 15,  8, 12, 14, 17, 14, 17, 13, 16,  2, 15,  1,  5,  6,  7,\n",
       "       19,  0,  2,  8, 17,  4,  8, 10,  3,  2, 11,  2,  9,  9,  5,  5,  4,\n",
       "        3, 17,  8, 10,  5,  3,  5,  5, 19,  6, 17,  4, 19,  9,  1,  1,  1,\n",
       "        1, 15, 11,  5,  8,  4, 13, 10,  7,  4,  0, 10,  5, 15,  0,  9,  0,\n",
       "        4, 12,  2,  9,  8,  4, 12, 10,  5, 16,  8, 10,  6,  2,  9,  4,  5,\n",
       "       12,  0, 19,  4,  2, 17,  9, 13,  6,  3, 17, 13,  2, 12])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_data = news_df.loc[:, 'News'].tolist()[:1000] # 시간 단축 - 1000개만\n",
    "y_data = news_df['Target'].tolist()[:1000]\n",
    "y = LabelEncoder().fit_transform(y_data)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
      "                                       ('multinomialnb', MultinomialNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'countvectorizer__lowercase': [True, False],\n",
      "                         'countvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'countvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                         'countvectorizer__stop_words': ['english'],\n",
      "                         'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ])},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
      "                                       ('bernoullinb', BernoulliNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'countvectorizer__lowercase': [True, False],\n",
      "                         'countvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'countvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                         'countvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
      "                                       ('multinomialnb', MultinomialNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'tfidfvectorizer__lowercase': [True, False],\n",
      "                         'tfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'tfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                         'tfidfvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
      "                                       ('bernoullinb', BernoulliNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'tfidfvectorizer__lowercase': [True, False],\n",
      "                         'tfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'tfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
      "                         'tfidfvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jung_yujin_1/miniconda3/envs/pknu_jin/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('stemmedcountvectorizer',\n",
      "                                        StemmedCountVectorizer()),\n",
      "                                       ('multinomialnb', MultinomialNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'stemmedcountvectorizer__lowercase': [True, False],\n",
      "                         'stemmedcountvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'stemmedcountvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'stemmedcountvectorizer__ngram_range': [(1, 1),\n",
      "                                                                 (1, 3)],\n",
      "                         'stemmedcountvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('stemmedcountvectorizer',\n",
      "                                        StemmedCountVectorizer()),\n",
      "                                       ('bernoullinb', BernoulliNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'stemmedcountvectorizer__lowercase': [True, False],\n",
      "                         'stemmedcountvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'stemmedcountvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'stemmedcountvectorizer__ngram_range': [(1, 1),\n",
      "                                                                 (1, 3)],\n",
      "                         'stemmedcountvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('stemmedtfidfvectorizer',\n",
      "                                        StemmedTfidfVectorizer()),\n",
      "                                       ('multinomialnb', MultinomialNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'multinomialnb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'stemmedtfidfvectorizer__lowercase': [True, False],\n",
      "                         'stemmedtfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'stemmedtfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'stemmedtfidfvectorizer__ngram_range': [(1, 1),\n",
      "                                                                 (1, 3)],\n",
      "                         'stemmedtfidfvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "GridSearchCV(cv=5,\n",
      "             estimator=Pipeline(steps=[('stemmedtfidfvectorizer',\n",
      "                                        StemmedTfidfVectorizer()),\n",
      "                                       ('bernoullinb', BernoulliNB())]),\n",
      "             n_jobs=36,\n",
      "             param_grid={'bernoullinb__alpha': array([1.  , 1.25, 1.5 , 1.75, 2.  ]),\n",
      "                         'stemmedtfidfvectorizer__lowercase': [True, False],\n",
      "                         'stemmedtfidfvectorizer__max_df': array([0.4       , 0.46666667, 0.53333333, 0.6       ]),\n",
      "                         'stemmedtfidfvectorizer__min_df': array([0.        , 0.03333333, 0.06666667, 0.1       ]),\n",
      "                         'stemmedtfidfvectorizer__ngram_range': [(1, 1),\n",
      "                                                                 (1, 3)],\n",
      "                         'stemmedtfidfvectorizer__stop_words': ['english']},\n",
      "             refit='accuracy', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "scoring = ['accuracy']\n",
    "estimator_results = []\n",
    "for i, (estimator, params) in enumerate(zip(pipelines,pipeline_params)):\n",
    "    gs_estimator = GridSearchCV(\n",
    "            refit=\"accuracy\", estimator=estimator, param_grid=params, scoring=scoring, cv=5, verbose=1, n_jobs=36)\n",
    "    print(gs_estimator)\n",
    "\n",
    "    gs_estimator.fit(X_data, y)\n",
    "    estimator_results.append(gs_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>min_df</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>max_df</th>\n",
       "      <th>binarize</th>\n",
       "      <th>alpha</th>\n",
       "      <th>ngram_rangemulti_class</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vectorizer model accuracy recall_macro precision_macro min_df lowercase  \\\n",
       "0        None  None     None         None            None   None      None   \n",
       "1        None  None     None         None            None   None      None   \n",
       "2        None  None     None         None            None   None      None   \n",
       "3        None  None     None         None            None   None      None   \n",
       "4        None  None     None         None            None   None      None   \n",
       "5        None  None     None         None            None   None      None   \n",
       "6        None  None     None         None            None   None      None   \n",
       "7        None  None     None         None            None   None      None   \n",
       "8        None  None     None         None            None   None      None   \n",
       "9        None  None     None         None            None   None      None   \n",
       "10       None  None     None         None            None   None      None   \n",
       "11       None  None     None         None            None   None      None   \n",
       "12       None  None     None         None            None   None      None   \n",
       "13       None  None     None         None            None   None      None   \n",
       "14       None  None     None         None            None   None      None   \n",
       "15       None  None     None         None            None   None      None   \n",
       "\n",
       "   max_df binarize alpha ngram_rangemulti_class penalty solver     C  \n",
       "0    None     None  None                   None    None   None  None  \n",
       "1    None     None  None                   None    None   None  None  \n",
       "2    None     None  None                   None    None   None  None  \n",
       "3    None     None  None                   None    None   None  None  \n",
       "4    None     None  None                   None    None   None  None  \n",
       "5    None     None  None                   None    None   None  None  \n",
       "6    None     None  None                   None    None   None  None  \n",
       "7    None     None  None                   None    None   None  None  \n",
       "8    None     None  None                   None    None   None  None  \n",
       "9    None     None  None                   None    None   None  None  \n",
       "10   None     None  None                   None    None   None  None  \n",
       "11   None     None  None                   None    None   None  None  \n",
       "12   None     None  None                   None    None   None  None  \n",
       "13   None     None  None                   None    None   None  None  \n",
       "14   None     None  None                   None    None   None  None  \n",
       "15   None     None  None                   None    None   None  None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "result_df_dict = {}\n",
    "result_attributes = [\"vectorizer\", \"model\", \"accuracy\", \"recall_macro\",\"precision_macro\" , \"min_df\", \n",
    "                     \"lowercase\", \"max_df\", \"binarize\", \"alpha\", \"ngram_range\"\n",
    "                     \"multi_class\", \"penalty\", \"solver\", \"C\"]\n",
    "\n",
    "pieline_list =  list(itertools.product(vectorizer_names, algorithm_names))\n",
    "\n",
    "for att in result_attributes:\n",
    "    result_df_dict[att] = [None for i in range(16)]\n",
    "\n",
    "result_df = DataFrame(result_df_dict)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer', 'multinomialnb'),\n",
       " ('countvectorizer', 'bernoullinb'),\n",
       " ('tfidfvectorizer', 'multinomialnb'),\n",
       " ('tfidfvectorizer', 'bernoullinb'),\n",
       " ('stemmedcountvectorizer', 'multinomialnb'),\n",
       " ('stemmedcountvectorizer', 'bernoullinb'),\n",
       " ('stemmedtfidfvectorizer', 'multinomialnb'),\n",
       " ('stemmedtfidfvectorizer', 'bernoullinb')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieline_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, estiamtor in enumerate(estimator_results):\n",
    "    best_estimator = estiamtor.best_estimator_\n",
    "    best_index = estiamtor.best_index_\n",
    "    result_df_dict[\"vectorizer\"][i] = pieline_list[i][0]\n",
    "    result_df_dict[\"model\"][i] = pieline_list[i][1]\n",
    "    result_df_dict[\"accuracy\"][i] = estiamtor.best_score_\n",
    "#     result_df_dict[\"recall_micro\"][i] = estiamtor.cv_results_[\"mean_test_recall_micro\"][best_index]\n",
    "#     result_df_dict[\"precision_micro\"][i] = estiamtor.cv_results_[\"mean_test_precision_micro\"][best_index]\n",
    "    for key, value in estiamtor.best_params_.items():\n",
    "        if key.split(\"__\")[1] in result_df_dict:\n",
    "            name = key.split(\"__\")[1]\n",
    "            result_df_dict[name][i] = value\n",
    "#     print(estiamtor.best_params_)\n",
    "#     print(a.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>min_df</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>max_df</th>\n",
       "      <th>binarize</th>\n",
       "      <th>alpha</th>\n",
       "      <th>ngram_rangemulti_class</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>countvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.675</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stemmedcountvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.662</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stemmedtfidfvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.630</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidfvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.622</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stemmedcountvectorizer</td>\n",
       "      <td>bernoullinb</td>\n",
       "      <td>0.396</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stemmedtfidfvectorizer</td>\n",
       "      <td>bernoullinb</td>\n",
       "      <td>0.396</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>countvectorizer</td>\n",
       "      <td>bernoullinb</td>\n",
       "      <td>0.343</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>None</td>\n",
       "      <td>1.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidfvectorizer</td>\n",
       "      <td>bernoullinb</td>\n",
       "      <td>0.343</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>None</td>\n",
       "      <td>1.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                vectorizer          model  accuracy recall_macro  \\\n",
       "0          countvectorizer  multinomialnb     0.675         None   \n",
       "4   stemmedcountvectorizer  multinomialnb     0.662         None   \n",
       "6   stemmedtfidfvectorizer  multinomialnb     0.630         None   \n",
       "2          tfidfvectorizer  multinomialnb     0.622         None   \n",
       "5   stemmedcountvectorizer    bernoullinb     0.396         None   \n",
       "7   stemmedtfidfvectorizer    bernoullinb     0.396         None   \n",
       "1          countvectorizer    bernoullinb     0.343         None   \n",
       "3          tfidfvectorizer    bernoullinb     0.343         None   \n",
       "8                     None           None       NaN         None   \n",
       "9                     None           None       NaN         None   \n",
       "10                    None           None       NaN         None   \n",
       "11                    None           None       NaN         None   \n",
       "12                    None           None       NaN         None   \n",
       "13                    None           None       NaN         None   \n",
       "14                    None           None       NaN         None   \n",
       "15                    None           None       NaN         None   \n",
       "\n",
       "   precision_macro    min_df lowercase    max_df binarize  alpha  \\\n",
       "0             None  0.000000      True  0.466667     None    1.0   \n",
       "4             None  0.000000      True  0.533333     None    1.0   \n",
       "6             None  0.000000      True  0.600000     None    1.0   \n",
       "2             None  0.000000      True  0.400000     None    1.0   \n",
       "5             None  0.033333      True  0.600000     None    1.0   \n",
       "7             None  0.033333      True  0.600000     None    1.0   \n",
       "1             None  0.033333      True  0.533333     None    1.5   \n",
       "3             None  0.033333      True  0.533333     None    1.5   \n",
       "8             None       NaN      None       NaN     None    NaN   \n",
       "9             None       NaN      None       NaN     None    NaN   \n",
       "10            None       NaN      None       NaN     None    NaN   \n",
       "11            None       NaN      None       NaN     None    NaN   \n",
       "12            None       NaN      None       NaN     None    NaN   \n",
       "13            None       NaN      None       NaN     None    NaN   \n",
       "14            None       NaN      None       NaN     None    NaN   \n",
       "15            None       NaN      None       NaN     None    NaN   \n",
       "\n",
       "   ngram_rangemulti_class penalty solver     C  \n",
       "0                    None    None   None  None  \n",
       "4                    None    None   None  None  \n",
       "6                    None    None   None  None  \n",
       "2                    None    None   None  None  \n",
       "5                    None    None   None  None  \n",
       "7                    None    None   None  None  \n",
       "1                    None    None   None  None  \n",
       "3                    None    None   None  None  \n",
       "8                    None    None   None  None  \n",
       "9                    None    None   None  None  \n",
       "10                   None    None   None  None  \n",
       "11                   None    None   None  None  \n",
       "12                   None    None   None  None  \n",
       "13                   None    None   None  None  \n",
       "14                   None    None   None  None  \n",
       "15                   None    None   None  None  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = DataFrame(result_df_dict, columns=result_attributes)\n",
    "result_df.sort_values(\"accuracy\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pknu_jin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d95c04b9551bfc1f42244394e44dd2408012d71311d6026dd90a6008fca68d0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
