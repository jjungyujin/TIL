{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM : 문자열 / 문자열\n",
    "- only text로 완성된 프롬프트\n",
    "- ex\\) GPT-3\n",
    "\n",
    "Chat model : chat message 리스트 / chat message\n",
    "- 연속된 대화를 담고있는 리스트\n",
    "- 대화(메세지)의 주체(system/human/ai)가 라벨링됨\n",
    "- ex\\) GPT-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[requirement]\n",
    "<bash>\n",
    "```\n",
    "pip install openai\n",
    "setx OPENAI_API_KEY “<yourkey>”\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n치킨 마요덮밥!\\n\\n재료 : 밥, 치킨, 마요네즈\\n\\n만드는 방법 : \\n1. 밥을 기름에 볶아줍니다.\\n2. 치킨을 기름에 볶아줍니다.\\n3. 마요네즈를 밥과 치킨에 뿌려줍니다.\\n4. 다시 기름에 볶아줍니다.\\n5. 완성!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"간단한 요리 하나 알려줘.\")\n",
    "# output : string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.generate([\"간단한 요리 하나 알려줘.\", \"여름철 과일 하나 알려줘.\"]*5)\n",
    "# output : LLMResult\n",
    "\n",
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\n스파게티 볶음! \\n\\n스파게티 볶음은 매운 맛을 좋아하는 사람들에게 인기가 많은 요리입니다. 스파게티 면, 고기, 야채, 토마토 소스, 마늘, 생강 등을 함께 볶아 만들 수 있습니다. 스파게티 볶음은 매운 맛이 잘 어울리는 요리', generation_info={'finish_reason': 'length', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n수박', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n김치볶음밥! 김치를 볶아서 밥과 함께 먹는 맛있는 요리입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n복숭아', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n김치볶음밥! 김치를 볶아서 밥과 함께 먹는 맛있는 요리입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n수박', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n비빔밥이에요! 밥, 무, 고추장, 참기름, 김치를 준비하시고 무는 잘 썰어주세요. 그리고 밥을 그릇에 담고 무와 고추장, 참기름을 넣고 잘 섞어주세요. 그리고 김치를 넣고 다시 잘 섞어주세요. 마지막으로 비빔', generation_info={'finish_reason': 'length', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n수박', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n치킨 마요덮밥!\\n\\n재료 : 밥, 치킨, 마요네즈\\n\\n만드는 방법 : \\n1. 밥을 기름에 볶아줍니다.\\n2. 치킨을 기름에 볶아줍니다.\\n3. 마요네즈를 밥과 치킨에 뿌려줍니다.\\n4. 다시 기름에 볶아줍니다.\\n5. 완성!', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n복숭아가 여름철에 좋은 과일입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 1272,\n",
       "  'prompt_tokens': 300,\n",
       "  'completion_tokens': 972},\n",
       " 'model_name': 'text-davinci-003'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\n치킨 마늘 양파 볶음!\\n\\n재료 : 치킨 한 개 (또는 한 세트), 마늘 한 개, 양파 한 개\\n\\n레시피 :\\n\\n1. 치킨을 잘게 썰어서 부침가루로 얇게 바삭하게 간합니다.\\n\\n2. 마늘, 양파를 잘게 다진 후 얇게 썰어 준비합니다.\\n\\n3. 샤워팬에 달', generation_info={'finish_reason': 'length', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n키위', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n피자 샐러드!\\n\\n피자를 썰어놓고 양상치, 사과, 씨앗과 함께 간장과 올리브유로 버무려 먹는 샐러드입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n감귤', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n< 돈까스 >\\n\\n돈까스는 손쉽게 만들 수 있고 맛있는 요리 중 하나입니다. \\n\\n큰 조리대가 필요 없고, 재료도 쉽게 챙길 수 있는 돈까스 요리법입니다. \\n\\n- 준비물: 스팸, 토마토, 새우, 양파, 당근, 식용유, 간장, 다진 마늘 \\n\\n-', generation_info={'finish_reason': 'length', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n바나나!', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n연어 요리를 해보시는 건 어떠세요?\\n\\n연어 요리는 매운 간장에 바삭바삭하게 볶은 연어를 곁들인 메뉴입니다. 준비 시간이 조금 걸리는 편이지만 생선맛이 강하게 남는 요리는 상당히 간단합니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n메론!', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n콩 볶음밥이 좋아요!\\n먼저 기름을 덜미 프라이팬에 두고 달걀을 한 개 깨어 넣은 다음 콩을 함께 볶으면 됩니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text='\\n\\n바나나 입니다!', generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_99_llm = OpenAI(temperature=0.99)\n",
    "temp_99_llm_result = temp_99_llm.generate([\"간단한 요리 하나 알려줘.\", \"여름철 과일 하나 알려줘.\"]*5)\n",
    "temp_99_llm_result.generations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HumanInputLLM (with wikipedia)  \n",
    "\n",
    "[requirement]  \n",
    "<bash> `pip install wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.human import HumanInputLLM\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\"])\n",
    "# llm = HumanInputLLM(\n",
    "#     prompt_func=lambda prompt: print(\n",
    "#         f\"\\n===PROMPT====\\n{prompt}\\n=====END OF PROMPT======\"\n",
    "#     )\n",
    "# )\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m This sounds like a phrase or something related to music.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Bocchi the Rock!\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Bocchi the Rock!\n",
      "Summary: Bocchi the Rock! (ぼっち・ざ・ろっく!, Botchi Za Rokku!) is a Japanese four-panel manga series written and illustrated by Aki Hamaji. It has been serialized in Houbunsha's seinen manga magazine Manga Time Kirara Max since December 2017. Its chapters have been collected in five tankōbon volumes as of November 2022.\n",
      "An anime television series adaptation produced by CloverWorks aired from October to December 2022. The series has been praised for its writing, comedy, characters, and depiction of social anxiety, with the anime's visual creativity receiving acclaim.\n",
      "\n",
      "Page: Kessoku Band (album)\n",
      "Summary: Kessoku Band (Japanese: 結束バンド, Hepburn: Kessoku Bando) is the debut studio album by Kessoku Band, a fictional musical group from the anime television series Bocchi the Rock!, released digitally on December 25, 2022, and physically on CD on December 28 by Aniplex. Featuring vocals from voice actresses Yoshino Aoyama, Sayumi Suzushiro, Saku Mizuno, and Ikumi Hasegawa, the album consists of 14 tracks previously heard in the anime, including a cover of Asian Kung-Fu Generation's \"Rockn' Roll, Morning Light Falls on You\", as well as newly recorded songs; nine singles preceded the album's physical release. Commercially, Kessoku Band peaked at number one on the Billboard Japan Hot Albums Chart and Oricon Albums Chart, and was certified gold by the Recording Industry Association of Japan.\n",
      "\n",
      "\n",
      "\n",
      "Page: Hitori Bocchi no Marumaru Seikatsu\n",
      "Summary: Hitori Bocchi no Marumaru Seikatsu (Japanese: ひとりぼっちの○○生活, lit. \"Bocchi Hitori's ____ Life\" or \"The ____ Life of Being Alone\") is a Japanese yonkoma manga series written and illustrated by Katsuwo. It was serialized in ASCII Media Works' Comic Dengeki Daioh \"g\" magazine from September 2013 to April 2021. Eight tankōbon volumes have been released. An anime television series adaptation by C2C aired from April to June 2019.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Bocchi the Rock! is a Japanese four-panel manga series written and illustrated by Aki Hamaji, which has been adapted into an anime television series produced by CloverWorks. It follows the story of a fictional musical group called Kessoku Band, and their album Kessoku Band was released in December 2022. It also has a spin-off manga series called Hitori Bocchi no Marumaru Seikatsu, which was adapted into an anime television series in 2019.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bocchi the Rock! is a Japanese four-panel manga series written and illustrated by Aki Hamaji, which has been adapted into an anime television series produced by CloverWorks. It follows the story of a fictional musical group called Kessoku Band, and their album Kessoku Band was released in December 2022. It also has a spin-off manga series called Hitori Bocchi no Marumaru Seikatsu, which was adapted into an anime television series in 2019.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is 'Bocchi the Rock!'?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 44\n",
      "\tPrompt Tokens: 5\n",
      "\tCompletion Tokens: 39\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00088\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke.\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import secret_loader\n",
    "\n",
    "SERPER = secret_loader.SecretLoader(\"Serper\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = SERPER[\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should research what products are best for summer skincare.\n",
      "Action: Search\n",
      "Action Input: \"summer skincare routine\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo Matter Your Skin Type, Apply Sunscreen Daily and Reassess Your Cleanser and Moisturizer ... All skin types benefit from a generous application ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look for more specific advice.\n",
      "Action: Search\n",
      "Action Input: \"summer skincare routine for oily skin\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.lorealparisusa.com/beauty-magazine/skin-care/skin-care-concerns/oily-skin-tips-for-summer\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: For oily skin, a summer skincare routine should include a gentle cleanser, a light moisturizer, and a generous application of sunscreen.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Total Token: 864, Total Cost: 0.01728\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"Recommend a summer skincare routine.\")\n",
    "\n",
    "print(f\"Total Token: {cb.total_tokens}, Total Cost: {cb.total_cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat models\n",
    "\n",
    "LLMs와 `call`, `generate`는 동일 (입력하는 `prompt` 형태만 다름)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\+ Streaming  \n",
    "사소하지만 사용자의 인내심을 위해 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a world of thirst, where the sun shines bright,\n",
      "There's a drink that brings delight, day or night,\n",
      "It's a bubbly sensation, a refreshing sensation,\n",
      "Let me sing to you about sparkling water's creation.\n",
      "\n",
      "(Chorus)\n",
      "Oh, sparkling water, you're a sparkling delight,\n",
      "Bubbles dancing in my glass, sparkling so bright,\n",
      "With every sip, you quench my thirst,\n",
      "Sparkling water, you're the one I choose first.\n",
      "\n",
      "(Verse 2)\n",
      "From mountain springs, you're born pure and clear,\n",
      "Effervescent bubbles, bringing joy near,\n",
      "A symphony of fizz, a melody so sweet,\n",
      "Sparkling water, you make my taste buds meet.\n",
      "\n",
      "(Chorus)\n",
      "Oh, sparkling water, you're a sparkling delight,\n",
      "Bubbles dancing in my glass, sparkling so bright,\n",
      "With every sip, you quench my thirst,\n",
      "Sparkling water, you're the one I choose first.\n",
      "\n",
      "(Bridge)\n",
      "When life gets heavy, and I need a lift,\n",
      "Sparkling water, you give my spirits a swift,\n",
      "You're a sparkling oasis, a moment of bliss,\n",
      "With your effervescence, I can't resist.\n",
      "\n",
      "(Verse 3)\n",
      "No need for sugar, no need for a fuss,\n",
      "Sparkling water, you're the drink for us,\n",
      "With flavors so diverse, from lime to berry,\n",
      "You bring a sparkle to life, so merry.\n",
      "\n",
      "(Chorus)\n",
      "Oh, sparkling water, you're a sparkling delight,\n",
      "Bubbles dancing in my glass, sparkling so bright,\n",
      "With every sip, you quench my thirst,\n",
      "Sparkling water, you're the one I choose first.\n",
      "\n",
      "(Outro)\n",
      "So raise your glass, let the bubbles rise,\n",
      "Sparkling water, you're a pleasant surprise,\n",
      "In a world of refreshment, you're the star,\n",
      "Sparkling water, you're the drink that goes far."
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "resp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a classic one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!"
     ]
    }
   ],
   "source": [
    "batch_messages = [[HumanMessage(content=\"Tell me a joke.\")]*3]\n",
    "batch_messages\n",
    "\n",
    "result = chat.generate(batch_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LLMResult' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'LLMResult' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {}, 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streaming하는 모델에서 generate할 떄 주의사항 !\n",
    "# token_usage를 알 수 없음\n",
    "\n",
    "result.llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Parser\n",
    "\n",
    "pydantic : parsing library\n",
    "- 출력 모델의 유형과 제약 보장\n",
    "- 데이터 유효성 검사를 통한 예기치 못한 오류 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Actor\\'s name\", \"type\": \"string\"}, \"film_name\": {\"title\": \"Film Name\", \"description\": \"list of names of films the actor has appeared in\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"name\", \"film_name\"]}\\n```\\nGenerate the filmography for a random actor.\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Actor(BaseModel):\n",
    "    name:str = Field(description=\"Actor's name\")\n",
    "    film_name: List[str] = Field(description=\"list of names of films the actor has appeared in\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "_input = prompt.format(query=actor_query)\n",
    "_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the user query.  \n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
    "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "Here is the output schema:\n",
    "```\n",
    "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Actor\\'s name\", \"type\": \"string\"}, \"film_name\": {\"title\": \"Film Name\", \"description\": \"list of names of films the actor has appeared in\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"name\", \"film_name\"]}\n",
    "```\n",
    "Generate the filmography for a random actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(name='Tom Hanks', film_name=['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Cast Away', 'Toy Story', 'A League of Their Own'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(_input)\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"name\": \"Tom Hanks\", \"film_name\": [\"Forrest Gump\", \"Saving Private Ryan\", \"The Green Mile\", \"Cast Away\", \"Toy Story\", \"A League of Their Own\"]}'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You're the best Korean cook.\\n    Give me a simple recipe for a student living on their own.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You're the best Korean cook.\n",
    "    Give me a simple recipe for {someone}.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(someone=\"a student living on their own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're the best Korean cook. Give me a simple recipe for a student living on their own.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "someone_input_prompt = PromptTemplate(input_variables=[\"someone\"], template=\"You're the best Korean cook. Give me a simple recipe for {someone}.\")\n",
    "someone_input_prompt.format(someone=\"a student living on their own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['someone'], ['someone'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt.input_variables, someone_input_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SystemMessagePromptTemplate(\n",
    "#     *,\n",
    "#     prompt: StringPromptTemplate,\n",
    "#     additional_kwargs: dict = dict\n",
    "# )\n",
    "    \n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"]\n",
    ")\n",
    "system_message_prompt2 = SystemMessagePromptTemplate(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SystemMessagePromptTemplate\nprompt\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m system_message_prompt2 \u001b[39m=\u001b[39m SystemMessagePromptTemplate(input_variables\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_language\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39moutput_language\u001b[39;49m\u001b[39m\"\u001b[39;49m], template\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful assistant that translates \u001b[39;49m\u001b[39m{input_language}\u001b[39;49;00m\u001b[39m to \u001b[39;49m\u001b[39m{output_language}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SystemMessagePromptTemplate\n",
      "prompt\n",
      "  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "# MessagePromptTemplate은 PromptTemplate이나 ChatPromptTemplate처럼 input_variabels, template으로 생성할 수는 없음\n",
    "system_message_prompt2 = SystemMessagePromptTemplate(input_variables=[\"input_language\", \"output_language\"], template=\"You are a helpful assistant that translates {input_language} to {output_language}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that translates Korean to English.', additional_kwargs={}), HumanMessage(content='오늘 저녁 뭐 먹지 ?', additional_kwargs={}, example=False)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "chat_prompt.format_prompt(input_language=\"Korean\", output_language=\"English\", text=\"오늘 저녁 뭐 먹지 ?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\+ Custom prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "    \n",
    "def get_source_code(function_name):\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "def hello_printer():\n",
    "    print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Given the function name and source code, generate an English language explanation of the function.\n",
    "        Function Name: {kwargs[\"function_name\"].__name__}\n",
    "        Source Code:\n",
    "        {source_code}\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Given the function name and source code, generate an English language explanation of the function.\n",
      "        Function Name: hello_printer\n",
      "        Source Code:\n",
      "        def hello_printer():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "prompt = fn_explainer.format(function_name=hello_printer)\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example selectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[requirement]\n",
    "<bash>\n",
    "```\n",
    "pip install chromadb\n",
    "pip install tiktoken\n",
    "```\n",
    "\n",
    "[error] chromadb fails to install on windows 10 and python 3.10  \n",
    "<a href=https://github.com/chroma-core/chroma/issues/189>download visual-cpp-build-tools</a>\n",
    "\n",
    "\n",
    "[study] What is Chroma ?  \n",
    "- open-source embedding database\n",
    "- makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, OpenAIEmbeddings(), Chroma, k=2\n",
    "    # 반가운 ? 임베딩 !\n",
    "    # k : 가져올 example 수 (k-shot)\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input.\",\n",
    "    suffix=\"Input : {adjective}\\nOutput :\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input.\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input : family\n",
      "Output :\n"
     ]
    }
   ],
   "source": [
    "print(similar_prompt.format(adjective=\"family\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input.\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input : car\n",
      "Output :\n"
     ]
    }
   ],
   "source": [
    "print(similar_prompt.format(adjective=\"car\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    max_length=9\n",
    "    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input.\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\", \n",
    "    input_variables=[\"adjective\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input.\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: big\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(adjective=\"big\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jungyujin2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jungyujin2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='VS code Extension\\n\\nVS code Extension ?\\n\\nVisual Studio Code의 확장 기능 (=플러그인)\\n다양한 기능으로 좋은 개발환경 구축 가능\\ncommand + shift + p 단축키로 검색 후 실행\\n\\nMaterial Theme\\n\\n테마 적용으로 가독성 향상\\n다양한 색상 중 개인이 원하는 테마 선택 가능\\n\\nBracket Pair Colorizer 2\\n\\n여러 괄호가 중첩되어 있을 경우 다른 색상을 부여하여 시각적 구분이 뚜렷해짐\\n정확도와 속도를 위해 버전 2 권장\\n\\nIndent-rainbow\\n\\n들여쓰기의 정도에 따라 색상 부여\\n\\nAuto Rename Tag\\n\\nHTML Tag를 변경 시 쌍을 이루던 나머지 Tag도 자동으로 변경\\n\\nLive Server\\n\\n파일 수정하고 저장한 경우 브라우저의 새로고침 없이 수정사항이 반영된 결과를 바로 확인 가능\\n\\nMaterial Icon Theme\\n\\n파일의 확장자에 따른 아이콘 부여\\n\\nGitignore\\n\\n.gitignore 파일에 추가될 만한 파일을 자동으로 넣어줌\\ngitignore 기능 실행 후 현재 다루는 작업 검색 (python, html, vue 등)', metadata={'source': 'data/VScode_Extension.md'})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import DirectoryLoader # .rst / .html 파일은 load 불가\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import JSONLoader\n",
    "# # !pip install unstructured --user\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "# pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "md_path = \"data/VScode_Extension.md\"\n",
    "loader = UnstructuredMarkdownLoader(md_path)\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain\n",
    "\n",
    "복잡한 퍼즐을 하나로 담아내어 퍼즐조각들을 연결시켜주는, 퍼즐판 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFuzzy Footsies.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"company\", \"product\"],\n",
    "    template=\"What is a good name for {company} that makes {product}?\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run({\n",
    "    'company': \"ABC Startup\",\n",
    "    'product': \"colorful socks\"\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM과 Prompt 뿐만 아니라 메모리도 추가 가능 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first three colors of a rainbow are red, orange, and yellow.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=OpenAI(),\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The next four colors of a rainbow are green, blue, indigo, and violet.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"And the next 4?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\+ Serialization : chain을 저장했다가 필요할 때 꺼내쓰기\n",
    "\n",
    "- 일부 chain에서만 지원\n",
    "- `.json` / `.yaml` 파일로 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.save(\"./data/llm_chain.json\")\n",
    "\n",
    "#llm_chain.prompt.save(\"./data/prompt.json\")\n",
    "#llm_chain.llm.save(\"./data/llm.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "type llm_chain.json\n",
    "```\n",
    "\n",
    "![image](data/cat_llm_chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Saving of memory is not yet supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conversation\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39m./data/conversation_chain.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:302\u001b[0m, in \u001b[0;36mChain.save\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m    299\u001b[0m directory_path\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    301\u001b[0m \u001b[39m# Fetch dictionary to save\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m chain_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdict()\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m save_path\u001b[39m.\u001b[39msuffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    305\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:276\u001b[0m, in \u001b[0;36mChain.dict\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return dictionary representation of chain.\"\"\"\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSaving of memory is not yet supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    277\u001b[0m _dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdict()\n\u001b[0;32m    278\u001b[0m _dict[\u001b[39m\"\u001b[39m\u001b[39m_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chain_type\n",
      "\u001b[1;31mValueError\u001b[0m: Saving of memory is not yet supported."
     ]
    }
   ],
   "source": [
    "conversation.save(\"./data/conversation_chain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain\n",
    "\n",
    "chain = load_chain(\"./data/llm_chain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the first color of a rainbow?\n",
      "\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' A rainbow is an arc of colors in the sky that is created when sunlight is refracted by raindrops. The colors of a rainbow, in order, are red, orange, yellow, green, blue, indigo, and violet. Therefore, the first color of a rainbow is red.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is the first color of a rainbow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the last one?\n",
      "\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The first step is to identify the problem. The second step is to brainstorm solutions. The third step is to evaluate the solutions. The fourth step is to choose the best solution. The fifth step is to implement the solution. The sixth step is to monitor and evaluate the results. The seventh and final step is to adjust the solution if needed.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is the last color?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "`Action Agent`\n",
    "- 각 time step에서 이전 작업을 기반으로 다음 작업 결정\n",
    "- 단기적 작업에 적합\n",
    "\n",
    "`Plan-and-execute Agent`\n",
    "- 전체 작업 순서를 계획하고 계획대로 실행\n",
    "- 장기적 작업에 적합\n",
    "\n",
    "Best\n",
    "- `Plan-and-execute Agent`가 계획을 실행할 때 `Action Agent`를 사용\n",
    "- 계획 능력 + 역동성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Agent\n",
    "\n",
    "Action Agent Type\n",
    "- `Zero-shot ReAct`\n",
    "- `Structured input ReAct`\n",
    "- `OpenAI Functions`\n",
    "- `Conversational`\n",
    "- `Self ask with search`\n",
    "- `ReAct document store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.\n",
      "Action: Search\n",
      "Action Input: \"Leo DiCaprio girlfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe actor is believed to have recently split from his girlfriend of five years, actor Camila Morrone, but has previously been romantically ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out Camila Morrone's age\n",
      "Action: Search\n",
      "Action Input: \"Camila Morrone age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m26 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate 26 raised to the 0.43 power\n",
      "Action: Calculator\n",
      "Action Input: 26^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4.059182145592686\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Leo DiCaprio's girlfriend is Camila Morrone and her current age raised to the 0.43 power is 4.059182145592686.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Leo DiCaprio's girlfriend is Camila Morrone and her current age raised to the 0.43 power is 4.059182145592686.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan-and-execute agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain import LLMMathChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "planner = load_chat_planner(model)\n",
    "executor = load_agent_executor(model, tools, verbose=True)\n",
    "            # tool을 사용할 수 있는, planner와 독립적인 agent\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
      "steps=[Step(value=\"Search for information about Leo DiCaprio's girlfriend.\"), Step(value='Find her current age.'), Step(value='Calculate her current age raised to the 0.43 power.'), Step(value=\"Given the above steps taken, respond to the user's original question.\\n\\n\")]\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction:\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Leo DiCaprio girlfriend\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Search for information about Leo DiCaprio's girlfriend.\n",
      "\n",
      "Response: Action:\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Leo DiCaprio girlfriend\"\n",
      "}\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the current age of Leo DiCaprio's girlfriend, I can search for information about her and see if her age is mentioned.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Leo DiCaprio girlfriend age\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHe's actually been spotted with 27-year-old supermodel Gigi Hadid and a 23-year-old actress named Victoria Lamas.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems that Leo DiCaprio has been spotted with both 27-year-old supermodel Gigi Hadid and 23-year-old actress Victoria Lamas. However, I couldn't find any specific information about the current age of Leo DiCaprio's girlfriend.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Find her current age.\n",
      "\n",
      "Response: It seems that Leo DiCaprio has been spotted with both 27-year-old supermodel Gigi Hadid and 23-year-old actress Victoria Lamas. However, I couldn't find any specific information about the current age of Leo DiCaprio's girlfriend.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the current age of Leo DiCaprio's girlfriend raised to the 0.43 power. I can use the calculator tool for this.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"pow(age_of_girlfriend, 0.43)\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "pow(age_of_girlfriend, 0.43)\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "pow(age_of_girlfriend, 0.43)\n",
      "```\n",
      "...numexpr.evaluate(\"pow(age_of_girlfriend, 0.43)\")...\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LLMMathChain._evaluate(\"\npow(age_of_girlfriend, 0.43)\n\") raised error: 'VariableNode' object is not callable. Please try again with a valid numerical expression",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:80\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[1;34m(self, expression)\u001b[0m\n\u001b[0;32m     78\u001b[0m     local_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpi\u001b[39m\u001b[39m\"\u001b[39m: math\u001b[39m.\u001b[39mpi, \u001b[39m\"\u001b[39m\u001b[39me\u001b[39m\u001b[39m\"\u001b[39m: math\u001b[39m.\u001b[39me}\n\u001b[0;32m     79\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m---> 80\u001b[0m         numexpr\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m     81\u001b[0m             expression\u001b[39m.\u001b[39;49mstrip(),\n\u001b[0;32m     82\u001b[0m             global_dict\u001b[39m=\u001b[39;49m{},  \u001b[39m# restrict access to globals\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m             local_dict\u001b[39m=\u001b[39;49mlocal_dict,  \u001b[39m# add common mathematical functions\u001b[39;49;00m\n\u001b[0;32m     84\u001b[0m         )\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\numexpr\\necompiler.py:817\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39mif\u001b[39;00m expr_key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _names_cache:\n\u001b[1;32m--> 817\u001b[0m     _names_cache[expr_key] \u001b[39m=\u001b[39m getExprNames(ex, context)\n\u001b[0;32m    818\u001b[0m names, ex_uses_vml \u001b[39m=\u001b[39m _names_cache[expr_key]\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\numexpr\\necompiler.py:704\u001b[0m, in \u001b[0;36mgetExprNames\u001b[1;34m(text, context)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetExprNames\u001b[39m(text, context):\n\u001b[1;32m--> 704\u001b[0m     ex \u001b[39m=\u001b[39m stringToExpression(text, {}, context)\n\u001b[0;32m    705\u001b[0m     ast \u001b[39m=\u001b[39m expressionToAST(ex)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\numexpr\\necompiler.py:289\u001b[0m, in \u001b[0;36mstringToExpression\u001b[1;34m(s, types, context)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m# now build the expression\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m ex \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(c, names)\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m expressions\u001b[39m.\u001b[39misConstant(ex):\n",
      "File \u001b[1;32m<expr>:1\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'VariableNode' object is not callable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWho is Leo DiCaprio\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms girlfriend? What is her current age raised to the 0.43 power?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\experimental\\plan_and_execute\\agent_executor.py:48\u001b[0m, in \u001b[0;36mPlanAndExecute._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     42\u001b[0m _new_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m     43\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprevious_steps\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_container,\n\u001b[0;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcurrent_step\u001b[39m\u001b[39m\"\u001b[39m: step,\n\u001b[0;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key],\n\u001b[0;32m     46\u001b[0m }\n\u001b[0;32m     47\u001b[0m new_inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_new_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs}\n\u001b[1;32m---> 48\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecutor\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     49\u001b[0m     new_inputs,\n\u001b[0;32m     50\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n\u001b[0;32m     53\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(\n\u001b[0;32m     54\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*****\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mStep: \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m     55\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\experimental\\plan_and_execute\\executors\\base.py:32\u001b[0m, in \u001b[0;36mChainExecutor.step\u001b[1;34m(self, inputs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m     29\u001b[0m     \u001b[39mself\u001b[39m, inputs: \u001b[39mdict\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StepResponse:\n\u001b[0;32m     31\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Take step.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mrun(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m StepResponse(response\u001b[39m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:239\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\agents\\agent.py:953\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 953\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m    954\u001b[0m         name_to_tool_map,\n\u001b[0;32m    955\u001b[0m         color_mapping,\n\u001b[0;32m    956\u001b[0m         inputs,\n\u001b[0;32m    957\u001b[0m         intermediate_steps,\n\u001b[0;32m    958\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    961\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[0;32m    962\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m    963\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\agents\\agent.py:820\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    818\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39mrun(\n\u001b[0;32m    821\u001b[0m         agent_action\u001b[39m.\u001b[39mtool_input,\n\u001b[0;32m    822\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    823\u001b[0m         color\u001b[39m=\u001b[39mcolor,\n\u001b[0;32m    824\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    825\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_run_kwargs,\n\u001b[0;32m    826\u001b[0m     )\n\u001b[0;32m    827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\tools\\base.py:294\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    293\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[0;32m    297\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    298\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\tools\\base.py:266\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    265\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[0;32m    267\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    268\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\tools\\base.py:409\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    408\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\n\u001b[0;32m    410\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[0;32m    411\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    412\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    413\u001b[0m     )\n\u001b[0;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[0;32m    415\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    416\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:149\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    143\u001b[0m _run_manager\u001b[39m.\u001b[39mon_text(inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key])\n\u001b[0;32m    144\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m    145\u001b[0m     question\u001b[39m=\u001b[39minputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key],\n\u001b[0;32m    146\u001b[0m     stop\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m```output\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    147\u001b[0m     callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child(),\n\u001b[0;32m    148\u001b[0m )\n\u001b[1;32m--> 149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_llm_result(llm_output, _run_manager)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:103\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[1;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m text_match:\n\u001b[0;32m    102\u001b[0m     expression \u001b[39m=\u001b[39m text_match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate_expression(expression)\n\u001b[0;32m    104\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer: \u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    105\u001b[0m     run_manager\u001b[39m.\u001b[39mon_text(output, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myellow\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\jungyujin2\\miniconda3\\envs\\genesis\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:87\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[1;34m(self, expression)\u001b[0m\n\u001b[0;32m     79\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m     80\u001b[0m         numexpr\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m     81\u001b[0m             expression\u001b[39m.\u001b[39mstrip(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m         )\n\u001b[0;32m     85\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLLMMathChain._evaluate(\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mexpression\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m) raised error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please try again with a valid numerical expression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[39m# Remove any leading and trailing brackets from the output\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m]$\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, output)\n",
      "\u001b[1;31mValueError\u001b[0m: LLMMathChain._evaluate(\"\npow(age_of_girlfriend, 0.43)\n\") raised error: 'VariableNode' object is not callable. Please try again with a valid numerical expression"
     ]
    }
   ],
   "source": [
    "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `> Entering new PlanAndExecute chain...` 에서 Plan의 결과가 먼저 출력\n",
    "2. Plan에 맞게 `> Entering new AgentExecutor chain...`가 연속적으로 실행"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이제 LangChain 으로 ,,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검색 도구를 활용하여 쿼리를 돌리고 최신 정보 수집 및 저장 가능\n",
    "- 계산기, wikipedia 등 외부 도구를 활용한 다양한 처리 가능\n",
    "- input과 output에 대한 control이 용이\n",
    "- 자유롭게 chat을 저장하고 필요할 때 꺼내쓰기 가능\n",
    "- 내 웹/앱 서비스에서 LLM을 활용하고 결과를 원하는 형태로 저장 및 공유 가능 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackLogs\n",
    "\n",
    "- `HumanInputLLM` 오류 해결\n",
    "- `Action Agent` type별 차이점"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
